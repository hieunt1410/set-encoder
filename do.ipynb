{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc09266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb20c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(doc):\n",
    "    doc = nlp(doc)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "def filter_document(doc, min_sentence_length=None):\n",
    "    sentences = get_sentences(doc)\n",
    "    if min_sentence_length:\n",
    "        sentences = [sent for sent in sentences\n",
    "                     if len(sent.split()) >= min_sentence_length]\n",
    "    doc = \" \".join(sentences)\n",
    "    return doc\n",
    "\n",
    "def preprocess_case_data(\n",
    "    text,\n",
    "    max_length=None,\n",
    "    min_sentence_length=None,\n",
    "    uncased=False,\n",
    "    filter_min_length=None,\n",
    "):\n",
    "\n",
    "    text = (\n",
    "        text.strip()\n",
    "        .replace(\"\\n\", \" \")\n",
    "        .replace(\"FRAGMENT_SUPPRESSED\", \"\")\n",
    "        .replace(\"FACTUAL\", \"\")\n",
    "        .replace(\"BACKGROUND\", \"\")\n",
    "        .replace(\"ORDER\", \"\")\n",
    "    )\n",
    "    if uncased:\n",
    "        text = text.lower()\n",
    "        \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    cite_number = re.search(r\"\\[[0-9]+\\]\", text)\n",
    "\n",
    "    if cite_number:\n",
    "        text = text[0: cite_number.span()[0]].strip() + ' ' + text[cite_number.span()[1] :].strip()\n",
    "        \n",
    "    if filter_min_length:\n",
    "        words = text.split()\n",
    "        if len(words) <= filter_min_length:\n",
    "            return None\n",
    "\n",
    "    if min_sentence_length:\n",
    "        text = filter_document(text, min_sentence_length)\n",
    "    if max_length:\n",
    "        words = text.split()[:max_length]\n",
    "        text = \" \".join(words)\n",
    "    if not text.endswith(\".\"):\n",
    "        text = text + \".\"\n",
    "    return text\n",
    "\n",
    "def create_data():\n",
    "    dataset = []\n",
    "    labels = json.load(open(\"data/task2_train_labels_2025.json\", \"r\"))\n",
    "\n",
    "    for case in sorted(os.listdir(\"data/task2_train_files_2025\")):\n",
    "        data = {}\n",
    "        for candidate in os.listdir(f\"data/task2_train_files_2025/{case}\"):\n",
    "            data[\"query_id\"] = case\n",
    "            data[\"entailed_fragment\"] = open(f\"data/task2_train_files_2025/{case}/entailed_fragment.txt\", \"r\").read()\n",
    "            data[\"doc_ids\"] = sorted(os.listdir(f\"data/task2_train_files_2025/{case}/paragraphs\"))\n",
    "            data[\"docs\"] = [preprocess_case_data(open(f\"data/task2_train_files_2025/{case}/paragraphs/{doc_id}\", \"r\").read()) for doc_id in data[\"doc_ids\"]]\n",
    "            data[\"qrels\"] = [{\n",
    "                \"query_id\": case,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"relevance\": int(doc_id in labels[case])\n",
    "            } for doc_id in data[\"doc_ids\"]]\n",
    "        dataset.append(data)\n",
    "    return dataset\n",
    "\n",
    "def create_data_2():\n",
    "    docs, queries, qrels = [], [], []\n",
    "    labels = json.load(open(\"data/task2_train_labels_2025.json\", \"r\"))\n",
    "\n",
    "    for case in sorted(os.listdir(\"data/task2_train_files_2025\")):\n",
    "        content = open(os.path.join(\"data/task2_train_files_2025\", case, \"entailed_fragment.txt\"), \"r\").read()\n",
    "        docs.append({\n",
    "            \"query_id\": case,\n",
    "            \"text\": content\n",
    "        })\n",
    "        \n",
    "        queries.extend([{\"query_id\": candidate, \"text\": preprocess_case_data(open(os.path.join(\"data/task2_train_files_2025\", case, \"paragraphs\", candidate), \"r\").read())} for candidate in sorted(os.listdir(os.path.join(\"data/task2_train_files_2025\", case, \"paragraphs\")))])\n",
    "        qrels.extend([f\"{case}\\t0\\t{candidate}\\t{1 if candidate in labels[case] else 0}\" for candidate in sorted(os.listdir(os.path.join(\"data/task2_train_files_2025\", case, \"paragraphs\")))])\n",
    "    return docs, queries, qrels\n",
    "\n",
    "docs, queries, qrels = create_data_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88cdac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/task2-collie-2025-docs.jsonl\", 'w') as f:\n",
    "    json.dump(docs, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "with open(\"data/task2-collie-2025-queries.jsonl\", 'w') as f:\n",
    "    json.dump(queries, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(\"data/task2-collie-2025-qrels.qrels\", 'w') as f:\n",
    "    f.write(\"\\n\".join(qrels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316394e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
