{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bc09266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x16ab55750>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efb20c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(doc):\n",
    "    doc = nlp(doc)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "def filter_document(doc, min_sentence_length=None):\n",
    "    sentences = get_sentences(doc)\n",
    "    if min_sentence_length:\n",
    "        sentences = [sent for sent in sentences\n",
    "                     if len(sent.split()) >= min_sentence_length]\n",
    "    doc = \" \".join(sentences)\n",
    "    return doc\n",
    "\n",
    "def preprocess_case_data(\n",
    "    text,\n",
    "    max_length=None,\n",
    "    min_sentence_length=None,\n",
    "    uncased=False,\n",
    "    filter_min_length=None,\n",
    "):\n",
    "\n",
    "    text = (\n",
    "        text.strip()\n",
    "        .replace(\"\\n\", \" \")\n",
    "        .replace(\"FRAGMENT_SUPPRESSED\", \"\")\n",
    "        .replace(\"FACTUAL\", \"\")\n",
    "        .replace(\"BACKGROUND\", \"\")\n",
    "        .replace(\"ORDER\", \"\")\n",
    "    )\n",
    "    if uncased:\n",
    "        text = text.lower()\n",
    "        \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    cite_number = re.search(r\"\\[[0-9]+\\]\", text)\n",
    "\n",
    "    if cite_number:\n",
    "        text = text[0: cite_number.span()[0]].strip() + ' ' + text[cite_number.span()[1] :].strip()\n",
    "        \n",
    "    if filter_min_length:\n",
    "        words = text.split()\n",
    "        if len(words) <= filter_min_length:\n",
    "            return None\n",
    "\n",
    "    if min_sentence_length:\n",
    "        text = filter_document(text, min_sentence_length)\n",
    "    if max_length:\n",
    "        words = text.split()[:max_length]\n",
    "        text = \" \".join(words)\n",
    "    if not text.endswith(\".\"):\n",
    "        text = text + \".\"\n",
    "    return text\n",
    "\n",
    "def create_data():\n",
    "    dataset = []\n",
    "    labels = json.load(open(\"data/task2_train_labels_2025.json\", \"r\"))\n",
    "\n",
    "    for case in sorted(os.listdir(\"data/task2_train_files_2025\")):\n",
    "        data = {}\n",
    "        for candidate in os.listdir(f\"data/task2_train_files_2025/{case}\"):\n",
    "            data[\"query_id\"] = case\n",
    "            data[\"entailed_fragment\"] = open(f\"data/task2_train_files_2025/{case}/entailed_fragment.txt\", \"r\").read()\n",
    "            data[\"doc_ids\"] = sorted(os.listdir(f\"data/task2_train_files_2025/{case}/paragraphs\"))\n",
    "            data[\"docs\"] = [preprocess_case_data(open(f\"data/task2_train_files_2025/{case}/paragraphs/{doc_id}\", \"r\").read()) for doc_id in data[\"doc_ids\"]]\n",
    "            data[\"qrels\"] = [{\n",
    "                \"query_id\": case,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"relevance\": int(doc_id in labels[case])\n",
    "            } for doc_id in data[\"doc_ids\"]]\n",
    "        dataset.append(data)\n",
    "    return dataset\n",
    "\n",
    "def create_data_2():\n",
    "    docs, queries, qrels = [], [], []\n",
    "    labels = json.load(open(\"data/task2_train_labels_2025.json\", \"r\"))\n",
    "\n",
    "    for case in sorted(os.listdir(\"data/task2_train_files_2025\")):\n",
    "        case_path = os.path.join(\"data/task2_train_files_2025\", case)\n",
    "        # Query: Use entailed_fragment.txt as query text\n",
    "        query_text = preprocess_case_data(\n",
    "            open(os.path.join(case_path, \"entailed_fragment.txt\"), \"r\").read()\n",
    "        )\n",
    "        if query_text:  # Ensure query text is not None after preprocessing\n",
    "            queries.append(f\"{case}\\t{query_text}\")\n",
    "        else:\n",
    "            print(f\"Warning: Skipping query for case {case} due to preprocessing returning None\")\n",
    "            continue\n",
    "\n",
    "        # Docs: Use paragraph texts as documents\n",
    "        paragraph_dir = os.path.join(case_path, \"paragraphs\")\n",
    "        for candidate in sorted(os.listdir(paragraph_dir)):\n",
    "            doc_text = preprocess_case_data(\n",
    "                open(os.path.join(paragraph_dir, candidate), \"r\").read()\n",
    "            )\n",
    "            if doc_text:  # Ensure doc text is not None\n",
    "                docs.append(f\"{candidate}\\t{doc_text}\")\n",
    "            else:\n",
    "                print(f\"Warning: Skipping doc {candidate} in case {case} due to preprocessing returning None\")\n",
    "\n",
    "            # Qrels: Map query_id (case) to doc_id (candidate) with relevance\n",
    "            relevance = 1 if candidate in labels[case] else 0\n",
    "            qrels.append(f\"{case}\\t{candidate}\\t{relevance}\")\n",
    "\n",
    "    return docs, queries, qrels\n",
    "\n",
    "docs, queries, qrels = create_data_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e88cdac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/task2-collie-2025-docs.tsv\", 'w') as f:\n",
    "    f.write(\"\\n\".join(docs))\n",
    "    \n",
    "with open(\"data/task2-collie-2025-queries.tsv\", 'w') as f:\n",
    "    f.write(\"\\n\".join(queries))\n",
    "\n",
    "with open(\"data/task2-collie-2025-qrels.qrels\", 'w') as f:\n",
    "    f.write(\"\\n\".join(qrels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b84714ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def validate_tsv(file_path, expected_columns=2):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for i, row in enumerate(reader, 1):\n",
    "            if len(row) != expected_columns:\n",
    "                print(f\"Error on line {i} in {file_path}: Expected {expected_columns} columns, got {len(row)}: {row}\")\n",
    "            if not row[0]:\n",
    "                print(f\"Error on line {i} in {file_path}: Missing doc_id\")\n",
    "            if len(row) > 1 and not row[1]:\n",
    "                print(f\"Error on line {i} in {file_path}: Missing text\")\n",
    "validate_tsv(\"data/task2-collie-2025-docs.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316394e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/hieungo/Downloads/train_bm25_scores.json\", \"r\") as f:\n",
    "    train_bm25_scores = json.load(f)\n",
    "with open(\"/Users/hieungo/Downloads/dev_bm25_scores.json\", \"r\") as f:\n",
    "    dev_bm25_scores = json.load(f)\n",
    "with open(\"/Users/hieungo/Downloads/test_bm25_scores.json\", \"r\") as f:\n",
    "    test_bm25_scores = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8d3d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all = {**train_bm25_scores, **dev_bm25_scores, **test_bm25_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1842bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for case, candidates in all.items():\n",
    "    scores.extend([f\"{case}\\t{candidate}\\t{score}\" for id, (candidate, score) in enumerate(candidates.items())])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40a307c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/task2-collie-2025-scoreddocs.tsv\", 'w') as f:\n",
    "    f.write(\"\\n\".join(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9b88b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
